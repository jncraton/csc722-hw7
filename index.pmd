% CSC722 HW 7
% Jon Craton

Problem 1
=========

> What is covariance of W and Z, C(W,Z) if  W = a X and Z = c X. (find your solution in terms of X)

Here is the definition of covariance:

$Cov(x,y) = {\sum_{i=1}^{n}(x_i - {\overline x})(y_i - {\overline y}) \over n}$

So we can represent our problem as:

$Cov(W,Z) = {\sum_{i=1}^{n}(W_i - {\overline W})(Z_i - {\overline Z}) \over n}$

$Cov(W,Z) = {\sum_{i=1}^{n}a(X_i - {\overline X})c(X_i - {\overline X}) \over n}$

We can factor out a and c:

$Cov(W,Z) = ac{\sum_{i=1}^{n}(X_i - {\overline X})(X_i - {\overline X}) \over n}$

Then simplify:

$Cov(W,Z) = ac{\sum_{i=1}^{n}(X_i - {\overline X})^2 \over n}$

Remembering our definition of variance, we can simplify further:

$Var(x) = {\sum_{i=1}^{n}(x_i - {\overline x})^2 \over n}$

$Cov(W,Z) = acVar(X)$

Problem 2
=========

> What is covariance of W and Z, C(W,Z), if W = a X + b Y and Z = c X + d Y. (in terms of X and Y)

Here is the definition of covariance:

$Cov(W,Z) = {\sum_{i=1}^{n}(W_i - {\overline W})(Z_i - {\overline Z}) \over n}$

Replacing W and Z:

$Cov(W,Z) = {\sum_{i=1}^{n}(aX_i + bY_i - a{\overline X} - b{\overline Y})(cX_i + dY_i - c{\overline X} + d{\overline Y}) \over n}$

We can factor a little:

$Cov(W,Z) = {\sum_{i=1}^{n}(aX_i - a{\overline X} + bY_i - b{\overline Y})(cX_i - c{\overline X} + dY_i - d{\overline Y}) \over n}$

$Cov(W,Z) = {\sum_{i=1}^{n}(a (X_i - {\overline X}) + b(Y_i - {\overline Y}))(c(X_i - {\overline X}) + d(Y_i - {\overline Y})) \over n}$

We can then multiply:

$Cov(W,Z) = {\sum_{i=1}^{n}(ac(X_i - {\overline X})^2 + ad(X_i - {\overline X})(Y_i - {\overline Y}) + bc(X_i - {\overline X})(Y_i - {\overline Y}) + bd(Y_i - {\overline Y})^2) \over n}$

Then separate out the terms:

$Cov(W,Z) = {\sum_{i=1}^{n}ac(X_i - {\overline X})^2 \over n} + 
{\sum_{i=1}^{n}ad(X_i - {\overline X})(Y_i - {\overline Y}) \over n} + 
{\sum_{i=1}^{n}bc(X_i - {\overline X})(Y_i - {\overline Y}) \over n} + 
{\sum_{i=1}^{n}bd(Y_i - {\overline Y})^2) \over n}$

And a bit more factoring:

$Cov(W,Z) = {ac\sum_{i=1}^{n}(X_i - {\overline X})^2 \over n} + 
{ad\sum_{i=1}^{n}(X_i - {\overline X})(Y_i - {\overline Y}) \over n} + 
{bc\sum_{i=1}^{n}bc(X_i - {\overline X})(Y_i - {\overline Y}) \over n} + 
{bd\sum_{i=1}^{n}(Y_i - {\overline Y})^2) \over n}$

Then remembering the following definitions:

$Var(x) = {\sum_{i=1}^{n}(x_i - {\overline x})^2 \over n}$

$Cov(x,y) = {\sum_{i=1}^{n}(x_i - {\overline x})(y_i - {\overline y}) \over n}$

$Cov(W,Z) = acVar(X) + adCov(X,Y) + bcCov(X,Y) + bdVar(Y)$

Problem 3
=========

> I throw a coin three times, I define X as number of heads for the first two flips and Y as the number of heads on the last two flips. Find C(X,Y)

Let's recall once again the definition of covariance:

$Cov(x,y) = {\sum_{i=1}^{n}(x_i - {\overline x})(y_i - {\overline y}) \over n}$

Let's enumerate possible outcomes, using 0 for tails and 1 for heads:

```python
from itertools import product

outcomes = list(product([0,1], repeat=3))
print(outcomes)
```

Then split this into X and Y and count numbers of heads:

```python
X = [sum(o[0:2]) for o in outcomes]
Y = [sum(o[1:3]) for o in outcomes]
```

Now we can calculate covariance:

```python, echo=False
def E(x):
  """
  Returns the mean of a list of numbers
  
  >>> E([1,2,3])
  2.0
  >>> E([1,1,1])
  1.0
  """

  return sum(x)/len(x)

def cov(x,y):
  """
  Returns the covariance of a list

  >>> cov([2,3,4],[3,4,6])
  1.0
  """

  return E([(xᵢ - E(x))*(yᵢ - E(y)) for xᵢ,yᵢ in zip(x,y)])
```

```python
cov(X,Y)
```

Problem 4
=========

> I have a random variable X that takes values -2,-1,0,1,2 each with probability .2. Let Y = X^2. What is C(X,Y).

Let's enumatre our outcomes:

```python
X = list(range(-2,3))
Y = [x**2 for x in X]
```

Then calculate covariance:

```python
cov(X,Y)
```

> Are X and Y independent?

Despite X and Y having a covariance of zero, they are obviously not independent based on our knowledge of their construction. Covariance will be zero for independent variables, but a covariance of zero does not definitively imply independence.

Problem 5
=========

> Find correlation coefficient, r(X,Y), if Y = a X+b, in both cases when a > 0 and when a < 0.

Recall the following definition of the Pearson correlation coefficient:

$\rho_{X,Y}= \frac{Cov(X,Y)}{\sigma_X \sigma_Y}$

Or, using variance directly in place of standard deviation:

$\rho_{X,Y}= \frac{Cov(X,Y)}{\sqrt{Var(X)} \sqrt{Var(Y)}}$

We can then use definitions for variance and covariance to expand this further:

$Var(x) = {\sum_{i=1}^{n}(x_i - {\overline x})^2 \over n}$

$Cov(x,y) = {\sum_{i=1}^{n}(x_i - {\overline x})(y_i - {\overline y}) \over n}$

$\rho_{X,Y}= 
\frac{
  {\sum_{i=1}^{n}(X_i - {\overline X})(Y_i - {\overline Y}) \over n}
}{
  \sqrt{{\sum_{i=1}^{n}(X_i - {\overline X})^2 \over n}} 
  \sqrt{{\sum_{i=1}^{n}(Y_i - {\overline Y})^2 \over n}}
}$

We can simplifiy the denominator somewhat:

$\rho_{X,Y}= 
\frac{
  {\sum_{i=1}^{n}(X_i - {\overline X})(Y_i - {\overline Y}) \over n}
}{
  {1 \over n}
  \sqrt{\sum_{i=1}^{n}(X_i - {\overline X})^2} 
  \sqrt{\sum_{i=1}^{n}(Y_i - {\overline Y})^2}
}$

Then multiplying by $n \over n$:

$\rho_{X,Y}= 
\frac{
  {\sum_{i=1}^{n}(X_i - {\overline X})(Y_i - {\overline Y})}
}{
  \sqrt{\sum_{i=1}^{n}(X_i - {\overline X})^2} 
  \sqrt{\sum_{i=1}^{n}(Y_i - {\overline Y})^2}
}$

Now let's plug in our value for Y:

$Y = a X+b$

$\rho_{X,Y}= 
\frac{
  {\sum_{i=1}^{n}(X_i - {\overline X})(aX_i + b - a {\overline X} - b)}
}{
  \sqrt{\sum_{i=1}^{n}(X_i - {\overline X})^2} 
  \sqrt{\sum_{i=1}^{n}(aX_i + b - a {\overline X} - b)^2}
}$

We see that our offset, $b$, simply drops out:

$\rho_{X,Y}= 
\frac{
  {\sum_{i=1}^{n}(X_i - {\overline X})(aX_i - a {\overline X})}
}{
  \sqrt{\sum_{i=1}^{n}(X_i - {\overline X})^2} 
  \sqrt{\sum_{i=1}^{n}(aX_i - a {\overline X})^2}
}$

We can now factor our scaling factor, $a$, out of the summations:

$\rho_{X,Y}= 
\frac{
  {\sum_{i=1}^{n}(X_i - {\overline X})a(X_i - {\overline X})}
}{
  \sqrt{\sum_{i=1}^{n}(X_i - {\overline X})^2} 
  \sqrt{\sum_{i=1}^{n}(a(X_i -  {\overline X}))^2}
}$

$\rho_{X,Y}= 
\frac{
  a{\sum_{i=1}^{n}(X_i - {\overline X})(X_i - {\overline X})}
}{
  \sqrt{\sum_{i=1}^{n}(X_i - {\overline X})^2} 
  \sqrt{\sum_{i=1}^{n}a^2(X_i -  {\overline X})^2}
}$

$\rho_{X,Y}= 
\frac{
  a{\sum_{i=1}^{n}(X_i - {\overline X})(X_i - {\overline X})}
}{
  |a|\sqrt{\sum_{i=1}^{n}(X_i - {\overline X})^2} 
  \sqrt{\sum_{i=1}^{n}(X_i -  {\overline X})^2}
}$

We can now simplify identical square roots:

$\rho_{X,Y}= 
\frac{
  a{\sum_{i=1}^{n}(X_i - {\overline X})(X_i - {\overline X})}
}{
  |a|\sqrt{\sum_{i=1}^{n}(X_i - {\overline X})^2} 
  \sqrt{\sum_{i=1}^{n}(X_i - {\overline X})^2}
}$

$\rho_{X,Y}= 
\frac{
  a{\sum_{i=1}^{n}(X_i - {\overline X})(X_i - {\overline X})}
}{
  |a|{\sum_{i=1}^{n}(X_i - {\overline X})^2} 
}$

We can them simplify down to an expression of just a:

$\rho_{X,Y}= 
\frac{
  a{\sum_{i=1}^{n}(X_i - {\overline X})^2}
}{
  |a|{\sum_{i=1}^{n}(X_i - {\overline X})^2} 
}$

$\rho_{X,Y}={a \over |a|}$

$\rho_{X,Y}=\begin{cases} 1, \mbox{if } a > 0 \\ -1, \mbox{if } a < 0\end{cases}$

This shows that correlation ignores both scaling and offset. It is simply a measure of direction and quality of linear relationships.
